name: Job Scraper Automation

on:
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:
    # Allow manual triggering
  push:
    # Run when code is pushed to main branch
    branches: [ main ]

jobs:
  # Setup job to determine which scrapers to run
  setup:
    runs-on: ubuntu-latest
    outputs:
      scrapers: ${{ steps.set-scrapers.outputs.scrapers }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: List scrapers
      id: set-scrapers
      run: |
        SCRAPERS=$(ls scripts/*_jobs_scraper.py | jq -R -s -c 'split("\n")[:-1]')
        echo "scrapers=$SCRAPERS" >> $GITHUB_OUTPUT
        echo "Found scrapers: $SCRAPERS"
  
  # Individual scraper jobs running in parallel
  scrape-jobs:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix:
        scraper: ${{ fromJson(needs.setup.outputs.scrapers) }}
      fail-fast: false
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Install Playwright browsers
      run: python -m playwright install --with-deps
      
    - name: Install xvfb
      run: sudo apt-get update && sudo apt-get install -y xvfb
      
    - name: Start Xvfb manually
      run: |
        export DISPLAY=:99
        Xvfb :99 -ac -screen 0 1920x1080x24 > /dev/null 2>&1 &
      shell: bash

    - name: Create jobs directory
      run: mkdir -p jobs

    - name: Run scraper
      env:
        DISPLAY: :99
      run: |
        echo "Running ${{ matrix.scraper }}..."
        python "${{ matrix.scraper }}" || echo "[!] ${{ matrix.scraper }} failed"
        
    - name: Verify scraper output
      run: |
        scraper_name=$(basename "${{ matrix.scraper }}" .py)
        company_name=$(echo "$scraper_name" | sed 's/_jobs_scraper//')
        expected_file="jobs/${company_name}_jobs_processed.json"
        
        if [ -f "$expected_file" ]; then
          echo "âœ… Output file created: $expected_file"
          echo "File size: $(du -h "$expected_file" | cut -f1)"
          job_count=$(jq length "$expected_file" 2>/dev/null || echo "invalid")
          echo "Job count: $job_count"
          
          # Validate JSON structure
          if jq empty "$expected_file" 2>/dev/null; then
            echo "âœ… JSON is valid"
          else
            echo "âŒ JSON is invalid"
            exit 1
          fi
        else
          echo "âŒ Expected output file not found: $expected_file"
          echo "Available files in jobs/:"
          ls -la jobs/ || echo "No jobs directory found"
          exit 1
        fi
      
    - name: Extract scraper filename and company
      id: scraper_info
      run: |
        scraper_file=$(basename '${{ matrix.scraper }}')
        company_name=$(echo "$scraper_file" | sed 's/_jobs_scraper\.py$//')
        echo "scraper_name=$scraper_file" >> $GITHUB_OUTPUT
        echo "company=$company_name" >> $GITHUB_OUTPUT
        echo "json_file=${company_name}_jobs_processed.json" >> $GITHUB_OUTPUT

    - name: Upload individual job data
      uses: actions/upload-artifact@v4
      with:
        name: job-data-${{ github.run_id }}-${{ steps.scraper_info.outputs.company }}
        path: jobs/${{ steps.scraper_info.outputs.json_file }}
        retention-days: 7
        if-no-files-found: warn

  # Job to process results and commit changes
  process-results:
    needs: scrape-jobs
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Clean existing jobs directory
      run: rm -rf jobs/
      
    - name: Create jobs directory
      run: mkdir -p jobs/
      
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: job-data-${{ github.run_id }}-*
        merge-multiple: false
        path: temp-artifacts/
        
    - name: Organize downloaded files
      run: |
        echo "Downloaded artifacts:"
        ls -la temp-artifacts/
        
        # Move all JSON files to jobs directory
        for artifact_dir in temp-artifacts/job-data-${{ github.run_id }}-*/; do
          if [ -d "$artifact_dir" ]; then
            echo "Processing artifact directory: $artifact_dir"
            # Move JSON files from artifact directory to jobs/
            find "$artifact_dir" -name "*.json" -exec mv {} jobs/ \;
          fi
        done
        
        echo "Final jobs directory contents:"
        ls -la jobs/
        
        # Add scraping metadata to each file for debugging
        echo "=== Adding scraping metadata ==="
        for file in jobs/*.json; do
          if [ -f "$file" ]; then
            # Add a comment with scraping timestamp (this will cause git to detect changes)
            echo "Adding metadata to $file"
            temp_file=$(mktemp)
            jq --arg timestamp "$(date -u +'%Y-%m-%d %H:%M:%S UTC')" \
               --arg run_id "${{ github.run_id }}" \
               '. + {"_scraping_metadata": {"last_scraped": $timestamp, "github_run_id": $run_id}}' \
               "$file" > "$temp_file" && mv "$temp_file" "$file"
          fi
        done
        
        # Clean up temporary artifacts directory
        rm -rf temp-artifacts/
        
    - name: List downloaded files
      run: |
        echo "Final job files:"
        ls -la jobs/
        echo ""
        echo "File sizes and job counts:"
        for file in jobs/*.json; do
          if [ -f "$file" ]; then
            filename=$(basename "$file")
            file_size=$(du -h "$file" | cut -f1)
            job_count=$(jq length "$file" 2>/dev/null || echo "invalid")
            echo "- $filename: $file_size ($job_count jobs)"
          fi
        done
        
    - name: Validate JSON files
      run: |
        echo "Validating JSON files..."
        for file in jobs/*.json; do
          if [ -f "$file" ]; then
            echo "Validating $file..."
            if jq empty "$file" 2>/dev/null; then
              echo "âœ… $file is valid JSON"
            else
              echo "âŒ $file is invalid JSON"
              exit 1
            fi
          fi
        done
        
    - name: Check for changes
      id: changes
      run: |
        echo "=== Checking for changes ==="
        echo "Files in jobs directory:"
        ls -la jobs/
        
        echo "=== File checksums before staging ==="
        find jobs/ -name "*.json" -exec sh -c 'echo "$1: $(md5sum "$1")"' _ {} \;
        
        git add jobs/
        
        echo "=== Git status after staging ==="
        git status
        
        echo "=== Files staged for commit ==="
        git diff --cached --name-only
        
        echo "=== Detailed diff ==="
        git diff --cached --stat
        
        if git diff --cached --quiet; then
          echo "no_changes=true" >> $GITHUB_OUTPUT
          echo "âŒ No changes detected in job files"
        else
          echo "no_changes=false" >> $GITHUB_OUTPUT
          echo "âœ… Changes detected in job files"
        fi
        
    - name: Commit and push changes
      if: steps.changes.outputs.no_changes == 'false'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add jobs/
        git commit -m "ğŸ¤– Auto-update job listings - $(date +'%Y-%m-%d %H:%M:%S UTC')"
        
        # Try to push with error handling
        if git push origin main; then
          echo "âœ… Successfully pushed changes to repository"
        else
          echo "âŒ Failed to push changes to repository"
          echo "Current git status:"
          git status
          echo "Recent commits:"
          git log --oneline -5
          exit 1
        fi
        
    - name: Generate summary report
      run: |
        echo "## ğŸ“Š Job Scraping Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ“ˆ Job Counts by Company:" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        total_jobs=0
        for file in jobs/*_jobs_processed.json; do
          if [ -f "$file" ]; then
            company=$(basename "$file" _jobs_processed.json | sed 's/.*/\u&/')
            count=$(jq length "$file" 2>/dev/null || echo "0")
            echo "- **$company**: $count jobs" >> $GITHUB_STEP_SUMMARY
            total_jobs=$((total_jobs + count))
          fi
        done
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ“Š Total Jobs: $total_jobs" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ“… Last Updated:" >> $GITHUB_STEP_SUMMARY
        echo "- $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.changes.outputs.no_changes }}" == "false" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… **Changes detected and committed to repository**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ğŸ“ Files Updated:" >> $GITHUB_STEP_SUMMARY
          for file in jobs/*.json; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              file_size=$(du -h "$file" | cut -f1)
              echo "- $filename ($file_size)" >> $GITHUB_STEP_SUMMARY
            fi
          done
        else
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "â„¹ï¸ **No new changes detected**" >> $GITHUB_STEP_SUMMARY
        fi
        
    - name: Upload combined job data
      uses: actions/upload-artifact@v4
      with:
        name: job-data-${{ github.run_id }}-combined
        path: jobs/
        retention-days: 7
        
  notify:
    needs: process-results
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Generate notification
      run: |
        if [ "${{ needs.process-results.result }}" == "success" ]; then
          echo "ğŸ‰ Job scraping completed successfully!"
          echo "ğŸ“Š Check the summary above for job counts."
        else
          echo "âŒ Job scraping failed!"
          echo "ğŸ” Check the logs for more details."
        fi