name: Job Scraper Automation

on:
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:
    # Allow manual triggering
  push:
    # Run when code is pushed to main branch
    branches: [ main ]

jobs:
  # Setup job to determine which scrapers to run
  setup:
    runs-on: ubuntu-latest
    outputs:
      scrapers: ${{ steps.set-scrapers.outputs.scrapers }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: List scrapers
      id: set-scrapers
      run: |
        SCRAPERS=$(ls scripts/*_jobs_scraper.py | jq -R -s -c 'split("\n")[:-1]')
        echo "scrapers=$SCRAPERS" >> $GITHUB_OUTPUT
        echo "Found scrapers: $SCRAPERS"
  
  # Individual scraper jobs running in parallel
  scrape-jobs:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix:
        scraper: ${{ fromJson(needs.setup.outputs.scrapers) }}
      fail-fast: false
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Install Playwright browsers
      run: python -m playwright install --with-deps
      
    - name: Install xvfb
      run: sudo apt-get update && sudo apt-get install -y xvfb
      
    - name: Start Xvfb manually
      run: |
        export DISPLAY=:99
        Xvfb :99 -ac -screen 0 1920x1080x24 > /dev/null 2>&1 &
      shell: bash

    - name: Create jobs directory
      run: mkdir -p jobs

    - name: Run scraper
      env:
        DISPLAY: :99
      run: |
        echo "Running ${{ matrix.scraper }}..."
        echo "Current working directory: $(pwd)"
        
        # Make sure jobs directory exists in project root
        mkdir -p jobs
        
        # Extract the scraper filename
        scraper_file=$(basename "${{ matrix.scraper }}")
        echo "Scraper file: $scraper_file"
        
        # Change to scripts directory since scrapers use ../jobs/ paths
        cd scripts/
        echo "Changed to scripts directory: $(pwd)"
        
        # Run the scraper from the scripts directory
        echo "Executing: python $scraper_file"
        python "$scraper_file" || echo "[!] $scraper_file failed"
        
        # Go back to project root to check results
        cd ..
        echo "Back to project root: $(pwd)"
        echo "Jobs directory after scraper:"
        ls -la jobs/
        
    - name: Verify scraper output
      run: |
        echo "Verifying scraper output..."
        
        # Extract company name from scraper path
        scraper_name=$(basename "${{ matrix.scraper }}" .py)
        company_name=$(echo "$scraper_name" | sed 's/_jobs_scraper//')
        expected_file="jobs/${company_name}_jobs_processed.json"
        
        echo "Looking for: $expected_file"
        
        if [ -f "$expected_file" ]; then
          echo "‚úÖ Output file created: $expected_file"
          echo "File size: $(du -h "$expected_file" | cut -f1)"
          job_count=$(jq length "$expected_file" 2>/dev/null || echo "invalid")
          echo "Job count: $job_count"
          
          # Validate JSON structure
          if jq empty "$expected_file" 2>/dev/null; then
            echo "‚úÖ JSON is valid"
          else
            echo "‚ùå JSON is invalid"
            exit 1
          fi
        else
          echo "‚ùå Expected output file not found: $expected_file"
          echo "Available files in jobs/:"
          ls -la jobs/ || echo "No jobs directory found"
          exit 1
        fi
      
    - name: Extract scraper filename and company
      id: scraper_info
      run: |
        scraper_file=$(basename '${{ matrix.scraper }}')
        company_name=$(echo "$scraper_file" | sed 's/_jobs_scraper\.py$//')
        echo "scraper_name=$scraper_file" >> $GITHUB_OUTPUT
        echo "company=$company_name" >> $GITHUB_OUTPUT
        echo "json_file=${company_name}_jobs_processed.json" >> $GITHUB_OUTPUT

    - name: Upload individual job data
      uses: actions/upload-artifact@v4
      with:
        name: job-data-${{ github.run_id }}-${{ steps.scraper_info.outputs.company }}
        path: jobs/${{ steps.scraper_info.outputs.json_file }}
        retention-days: 7
        if-no-files-found: warn

  # Job to process results and commit changes
  process-results:
    needs: scrape-jobs
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Clean existing jobs directory
      run: rm -rf jobs/
      
    - name: Create jobs directory
      run: mkdir -p jobs/
      
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: job-data-${{ github.run_id }}-*
        merge-multiple: false
        path: temp-artifacts/
        
    - name: Organize downloaded files
      run: |
        echo "Downloaded artifacts:"
        ls -la temp-artifacts/
        
        # Move all JSON files to jobs directory
        for artifact_dir in temp-artifacts/job-data-${{ github.run_id }}-*/; do
          if [ -d "$artifact_dir" ]; then
            echo "Processing artifact directory: $artifact_dir"
            # Move JSON files from artifact directory to jobs/
            find "$artifact_dir" -name "*.json" -exec mv {} jobs/ \;
          fi
        done
        
        echo "Final jobs directory contents:"
        ls -la jobs/
        
        # Add scraping metadata to each file for debugging
        echo "=== Adding scraping metadata ==="
        echo "‚ö†Ô∏è  Skipping JSON modification to preserve array structure for frontend compatibility"
        echo "üìù Using alternative approach with timestamp file"
        
        # Count jobs in each file for reporting
        for file in jobs/*.json; do
          if [ -f "$file" ]; then
            if jq empty "$file" 2>/dev/null; then
              job_count=$(jq length "$file" 2>/dev/null || echo "unknown")
              echo "üìä $file: $job_count jobs"
            else
              echo "‚ùå $file: invalid JSON"
            fi
          fi
        done
        
        echo "=== Final file checksums after metadata addition ==="
        find jobs/ -name "*.json" -exec sh -c 'echo "$1: $(md5sum "$1")"' _ {} \;
        
        # Fallback: Create a comprehensive timestamp file to ensure changes are detected
        echo "=== Creating timestamp file as fallback ==="
        timestamp_file="jobs/.last_updated"
        
        echo "# Job Scraper Run Information" > "$timestamp_file"
        echo "Last updated: $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> "$timestamp_file"
        echo "GitHub Run ID: ${{ github.run_id }}" >> "$timestamp_file"
        echo "Workflow: ${{ github.workflow }}" >> "$timestamp_file"
        echo "Trigger: ${{ github.event_name }}" >> "$timestamp_file"
        echo "Branch: ${{ github.ref_name }}" >> "$timestamp_file"
        echo "" >> "$timestamp_file"
        
        echo "# Job Counts by Company:" >> "$timestamp_file"
        total_jobs=0
        for file in jobs/*.json; do
          if [ -f "$file" ]; then
            filename=$(basename "$file" _jobs_processed.json)
            if jq empty "$file" 2>/dev/null; then
              job_count=$(jq length "$file" 2>/dev/null || echo "0")
              echo "$filename: $job_count jobs" >> "$timestamp_file"
              total_jobs=$((total_jobs + job_count))
            else
              echo "$filename: invalid JSON" >> "$timestamp_file"
            fi
          fi
        done
        
        echo "" >> "$timestamp_file"
        echo "Total jobs across all companies: $total_jobs" >> "$timestamp_file"
        
        echo "‚úÖ Created comprehensive timestamp file: $timestamp_file"
        echo "üìÑ Timestamp file content:"
        cat "$timestamp_file"
        
        # Clean up temporary artifacts directory
        rm -rf temp-artifacts/
        
    - name: List downloaded files
      run: |
        echo "Final job files:"
        ls -la jobs/
        echo ""
        echo "File sizes and job counts:"
        for file in jobs/*.json; do
          if [ -f "$file" ]; then
            filename=$(basename "$file")
            file_size=$(du -h "$file" | cut -f1)
            job_count=$(jq length "$file" 2>/dev/null || echo "invalid")
            echo "- $filename: $file_size ($job_count jobs)"
          fi
        done
        
    - name: Validate JSON files
      run: |
        echo "Validating JSON files..."
        for file in jobs/*.json; do
          if [ -f "$file" ]; then
            echo "Validating $file..."
            if jq empty "$file" 2>/dev/null; then
              echo "‚úÖ $file is valid JSON"
            else
              echo "‚ùå $file is invalid JSON"
              exit 1
            fi
          fi
        done
        
    - name: Check for changes
      id: changes
      run: |
        echo "=== Checking for changes ==="
        echo "Files in jobs directory:"
        ls -la jobs/
        
        echo "=== File checksums before staging ==="
        find jobs/ -name "*.json" -exec sh -c 'echo "$1: $(md5sum "$1")"' _ {} \;
        
        # Also show the timestamp file if it exists
        if [ -f "jobs/.last_updated" ]; then
          echo "Timestamp file content:"
          cat jobs/.last_updated
        fi
        
        # Stage all files in jobs directory (including .last_updated)
        git add jobs/
        
        echo "=== Git status after staging ==="
        git status
        
        echo "=== Files staged for commit ==="
        git diff --cached --name-only
        
        echo "=== Detailed diff ==="
        git diff --cached --stat
        
        # Check if there are any staged changes
        if git diff --cached --quiet; then
          echo "no_changes=true" >> $GITHUB_OUTPUT
          echo "‚ùå No changes detected in job files"
        else
          echo "no_changes=false" >> $GITHUB_OUTPUT
          echo "‚úÖ Changes detected in job files"
          echo "Files that changed:"
          git diff --cached --name-only
        fi
        
    - name: Commit and push changes
      if: steps.changes.outputs.no_changes == 'false'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add jobs/
        git commit -m "ü§ñ Auto-update job listings - $(date +'%Y-%m-%d %H:%M:%S UTC')"
        
        # Try to push with error handling
        if git push origin main; then
          echo "‚úÖ Successfully pushed changes to repository"
        else
          echo "‚ùå Failed to push changes to repository"
          echo "Current git status:"
          git status
          echo "Recent commits:"
          git log --oneline -5
          exit 1
        fi
        
    - name: Generate summary report
      run: |
        echo "## üìä Job Scraping Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üìà Job Counts by Company:" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        total_jobs=0
        for file in jobs/*_jobs_processed.json; do
          if [ -f "$file" ]; then
            company=$(basename "$file" _jobs_processed.json | sed 's/.*/\u&/')
            count=$(jq length "$file" 2>/dev/null || echo "0")
            echo "- **$company**: $count jobs" >> $GITHUB_STEP_SUMMARY
            total_jobs=$((total_jobs + count))
          fi
        done
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üìä Total Jobs: $total_jobs" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üìÖ Last Updated:" >> $GITHUB_STEP_SUMMARY
        echo "- $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.changes.outputs.no_changes }}" == "false" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "‚úÖ **Changes detected and committed to repository**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìù Files Updated:" >> $GITHUB_STEP_SUMMARY
          for file in jobs/*.json; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              file_size=$(du -h "$file" | cut -f1)
              echo "- $filename ($file_size)" >> $GITHUB_STEP_SUMMARY
            fi
          done
        else
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "‚ÑπÔ∏è **No new changes detected**" >> $GITHUB_STEP_SUMMARY
        fi
        
    - name: Upload combined job data
      uses: actions/upload-artifact@v4
      with:
        name: job-data-${{ github.run_id }}-combined
        path: jobs/
        retention-days: 7
        
  notify:
    needs: process-results
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Generate notification
      run: |
        if [ "${{ needs.process-results.result }}" == "success" ]; then
          echo "üéâ Job scraping completed successfully!"
          echo "üìä Check the summary above for job counts."
        else
          echo "‚ùå Job scraping failed!"
          echo "üîç Check the logs for more details."
        fi