name: Job Scraper Automation

on:
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:
    # Allow manual triggering
  push:
    # Run when code is pushed to main branch
    branches: [ main ]

jobs:
  # Setup job to determine which scrapers to run
  setup:
    runs-on: ubuntu-latest
    outputs:
      scrapers: ${{ steps.set-scrapers.outputs.scrapers }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: List scrapers
      id: set-scrapers
      run: |
        SCRAPERS=$(ls scripts/*_jobs_scraper.py | jq -R -s -c 'split("\n")[:-1]')
        echo "scrapers=$SCRAPERS" >> $GITHUB_OUTPUT
        echo "Found scrapers: $SCRAPERS"
  
  # Individual scraper jobs running in parallel
  scrape-jobs:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix:
        scraper: ${{ fromJson(needs.setup.outputs.scrapers) }}
      fail-fast: false
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Install Playwright browsers
      run: python -m playwright install --with-deps
      
    - name: Install xvfb
      run: sudo apt-get update && sudo apt-get install -y xvfb
      
    - name: Start Xvfb manually
      run: |
        export DISPLAY=:99
        Xvfb :99 -ac -screen 0 1920x1080x24 > /dev/null 2>&1 &
      shell: bash

    - name: Create jobs directory
      run: mkdir -p jobs

    - name: Run scraper
      env:
        DISPLAY: :99
      run: |
        echo "Running ${{ matrix.scraper }}..."
        python "${{ matrix.scraper }}" || echo "[!] ${{ matrix.scraper }} failed"
      
    - name: Extract scraper filename
      id: scraper_name
      run: echo "name=$(basename '${{ matrix.scraper }}')" >> $GITHUB_OUTPUT

    - name: Upload individual job data
      uses: actions/upload-artifact@v4
      with:
        name: job-data-${{ github.run_id }}-${{ steps.scraper_name.outputs.name }}
        path: jobs/
        retention-days: 7

  # Job to process results and commit changes
  process-results:
    needs: scrape-jobs
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: job-data-${{ github.run_id }}-*
        merge-multiple: true
        path: jobs/
    
    - name: Check for changes
      id: changes
      run: |
        if git diff --quiet jobs/; then
          echo "no_changes=true" >> $GITHUB_OUTPUT
        else
          echo "no_changes=false" >> $GITHUB_OUTPUT
        fi
        
    - name: Configure Git
      if: steps.changes.outputs.no_changes == 'false'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
    - name: Commit and push changes
      if: steps.changes.outputs.no_changes == 'false'
      run: |
        git add jobs/
        git commit -m "🤖 Auto-update job listings - $(date +'%Y-%m-%d %H:%M:%S UTC')"
        git push
        
    - name: Generate summary report
      run: |
        echo "## 📊 Job Scraping Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 📈 Job Counts by Company:" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        for file in jobs/*_jobs_processed.json; do
          if [ -f "$file" ]; then
            company=$(basename "$file" _jobs_processed.json | sed 's/.*/\u&/')
            count=$(jq length "$file" 2>/dev/null || echo "0")
            echo "- **$company**: $count jobs" >> $GITHUB_STEP_SUMMARY
          fi
        done
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 📅 Last Updated:" >> $GITHUB_STEP_SUMMARY
        echo "- $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.changes.outputs.no_changes }}" == "false" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "✅ **Changes detected and committed to repository**" >> $GITHUB_STEP_SUMMARY
        else
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ℹ️ **No new changes detected**" >> $GITHUB_STEP_SUMMARY
        fi
        
    - name: Upload combined job data
      uses: actions/upload-artifact@v4
      with:
        name: job-data-${{ github.run_id }}-combined
        path: jobs/
        retention-days: 7
        
  notify:
    needs: process-results
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Generate notification
      run: |
        if [ "${{ needs.process-results.result }}" == "success" ]; then
          echo "🎉 Job scraping completed successfully!"
          echo "📊 Check the summary above for job counts."
        else
          echo "❌ Job scraping failed!"
          echo "🔍 Check the logs for more details."
        fi