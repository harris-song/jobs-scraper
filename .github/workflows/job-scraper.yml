name: Job Scraper Automation

on:
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:
    # Allow manual triggering
  push:
    # Run when code is pushed to main branch
    branches: [ main ]

jobs:
  # Setup job to determine which scrapers to run
  setup:
    runs-on: ubuntu-latest
    outputs:
      scrapers: ${{ steps.set-scrapers.outputs.scrapers }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: List scrapers
      id: set-scrapers
      run: |
        SCRAPERS=$(ls scripts/*_jobs_scraper.py | jq -R -s -c 'split("\n")[:-1]')
        echo "scrapers=$SCRAPERS" >> $GITHUB_OUTPUT
        echo "Found scrapers: $SCRAPERS"
  
  # Individual scraper jobs running in parallel
  scrape-jobs:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix:
        scraper: ${{ fromJson(needs.setup.outputs.scrapers) }}
      fail-fast: false
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Install Playwright browsers
      run: python -m playwright install --with-deps
      
    - name: Install xvfb
      run: sudo apt-get update && sudo apt-get install -y xvfb
      
    - name: Start Xvfb manually
      run: |
        export DISPLAY=:99
        Xvfb :99 -ac -screen 0 1920x1080x24 > /dev/null 2>&1 &
      shell: bash

    - name: Create jobs directory
      run: mkdir -p jobs

    - name: Run scraper
      env:
        DISPLAY: :99
      run: |
        echo "Running ${{ matrix.scraper }}..."
        python "${{ matrix.scraper }}" || echo "[!] ${{ matrix.scraper }} failed"
      
    - name: Extract scraper filename
      id: scraper_name
      run: echo "name=$(basename '${{ matrix.scraper }}')" >> $GITHUB_OUTPUT

    - name: Upload individual job data
      uses: actions/upload-artifact@v4
      with:
        name: job-data-${{ github.run_id }}-${{ steps.scraper_name.outputs.name }}
        path: jobs/
        retention-days: 7

  # Job to process results and commit changes
  process-results:
    needs: scrape-jobs
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: job-data-${{ github.run_id }}-*
        merge-multiple: true
        path: jobs/
    
    - name: Check for changes
      id: changes
      run: |
        if git diff --quiet jobs/; then
          echo "no_changes=true" >> $GITHUB_OUTPUT
        else
          echo "no_changes=false" >> $GITHUB_OUTPUT
        fi
        
    - name: Configure Git
      if: steps.changes.outputs.no_changes == 'false'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
    - name: Commit and push changes
      if: steps.changes.outputs.no_changes == 'false'
      run: |
        git add jobs/
        git commit -m "ğŸ¤– Auto-update job listings - $(date +'%Y-%m-%d %H:%M:%S UTC')"
        git push
        
    - name: Generate summary report
      run: |
        echo "## ğŸ“Š Job Scraping Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ“ˆ Job Counts by Company:" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        for file in jobs/*_jobs_processed.json; do
          if [ -f "$file" ]; then
            company=$(basename "$file" _jobs_processed.json | sed 's/.*/\u&/')
            count=$(jq length "$file" 2>/dev/null || echo "0")
            echo "- **$company**: $count jobs" >> $GITHUB_STEP_SUMMARY
          fi
        done
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ“… Last Updated:" >> $GITHUB_STEP_SUMMARY
        echo "- $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.changes.outputs.no_changes }}" == "false" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… **Changes detected and committed to repository**" >> $GITHUB_STEP_SUMMARY
        else
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "â„¹ï¸ **No new changes detected**" >> $GITHUB_STEP_SUMMARY
        fi
        
    - name: Upload combined job data
      uses: actions/upload-artifact@v4
      with:
        name: job-data-${{ github.run_id }}-combined
        path: jobs/
        retention-days: 7
        
  notify:
    needs: process-results
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Generate notification
      run: |
        if [ "${{ needs.process-results.result }}" == "success" ]; then
          echo "ğŸ‰ Job scraping completed successfully!"
          echo "ğŸ“Š Check the summary above for job counts."
        else
          echo "âŒ Job scraping failed!"
          echo "ğŸ” Check the logs for more details."
        fi