name: Job Scraper Automation

on:
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:
    # Allow manual triggering
  push:
    # Run when code is pushed to main branch
    branches: [ main ]

jobs:
  # Setup job to determine which scrapers to run
  setup:
    runs-on: ubuntu-latest
    outputs:
      scrapers: ${{ steps.set-scrapers.outputs.scrapers }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: List scrapers
      id: set-scrapers
      run: |
        SCRAPERS=$(ls scripts/*_jobs_scraper.py | jq -R -s -c 'split("\n")[:-1]')
        echo "scrapers=$SCRAPERS" >> $GITHUB_OUTPUT
        echo "Found scrapers: $SCRAPERS"
  
  # Individual scraper jobs running in parallel
  scrape-jobs:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix:
        scraper: ${{ fromJson(needs.setup.outputs.scrapers) }}
      fail-fast: false
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Install Playwright browsers
      run: python -m playwright install --with-deps
      
    - name: Install xvfb
      run: sudo apt-get update && sudo apt-get install -y xvfb
      
    - name: Start Xvfb manually
      run: |
        export DISPLAY=:99
        Xvfb :99 -ac -screen 0 1920x1080x24 > /dev/null 2>&1 &
      shell: bash

    - name: Create jobs directory
      run: mkdir -p jobs

    - name: Run scraper
      env:
        DISPLAY: :99
      run: |
        # Make sure jobs directory exists in project root
        mkdir -p jobs
        
        # Extract the scraper filename and change to scripts directory
        scraper_file=$(basename "${{ matrix.scraper }}")
        cd scripts/
        
        # Run the scraper from the scripts directory
        echo "Running $scraper_file..."
        python "$scraper_file" || echo "[!] $scraper_file failed"
        
    - name: Verify scraper output
      run: |
        # Extract company name from scraper path
        scraper_name=$(basename "${{ matrix.scraper }}" .py)
        company_name=$(echo "$scraper_name" | sed 's/_jobs_scraper//')
        expected_file="jobs/${company_name}_jobs_processed.json"
        
        if [ -f "$expected_file" ]; then
          job_count=$(jq length "$expected_file" 2>/dev/null || echo "0")
          echo "✅ $company_name: $job_count jobs"
          
          # Validate JSON structure
          if ! jq empty "$expected_file" 2>/dev/null; then
            echo "❌ Invalid JSON in $expected_file"
            exit 1
          fi
        else
          echo "❌ Expected output file not found: $expected_file"
          exit 1
        fi
      
    - name: Extract scraper filename and company
      id: scraper_info
      run: |
        scraper_file=$(basename '${{ matrix.scraper }}')
        company_name=$(echo "$scraper_file" | sed 's/_jobs_scraper\.py$//')
        echo "scraper_name=$scraper_file" >> $GITHUB_OUTPUT
        echo "company=$company_name" >> $GITHUB_OUTPUT
        echo "json_file=${company_name}_jobs_processed.json" >> $GITHUB_OUTPUT

    - name: Upload individual job data
      uses: actions/upload-artifact@v4
      with:
        name: job-data-${{ github.run_id }}-${{ steps.scraper_info.outputs.company }}
        path: jobs/${{ steps.scraper_info.outputs.json_file }}
        retention-days: 7
        if-no-files-found: warn

  # Job to process results and commit changes
  process-results:
    needs: scrape-jobs
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Clean existing jobs directory
      run: rm -rf jobs/
      
    - name: Create jobs directory
      run: mkdir -p jobs/
      
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: job-data-${{ github.run_id }}-*
        merge-multiple: false
        path: temp-artifacts/
        
    - name: Organize downloaded files
      run: |
        # Move all JSON files to jobs directory
        for artifact_dir in temp-artifacts/job-data-${{ github.run_id }}-*/; do
          if [ -d "$artifact_dir" ]; then
            find "$artifact_dir" -name "*.json" -exec mv {} jobs/ \;
          fi
        done
        
        # Add job count summary for logging
        echo "=== Job Summary ==="
        for file in jobs/*.json; do
          if [ -f "$file" ]; then
            company=$(basename "$file" _jobs_processed.json)
            job_count=$(jq length "$file" 2>/dev/null || echo "0")
            echo "$company: $job_count jobs"
          fi
        done
        
        # Add scraping metadata to each file for debugging
        echo "=== Adding Timestamp Metadata ==="
        echo "📝 Preserving JSON array structure for frontend compatibility"
        
        # Count jobs in each file for reporting
        for file in jobs/*.json; do
          if [ -f "$file" ]; then
            if jq empty "$file" 2>/dev/null; then
              job_count=$(jq length "$file" 2>/dev/null || echo "unknown")
              echo "📊 $(basename "$file"): $job_count jobs"
            else
              echo "❌ $(basename "$file"): invalid JSON"
            fi
          fi
        done
        
        echo "=== Final file checksums after metadata addition ==="
        find jobs/ -name "*.json" -exec sh -c 'echo "$(basename "$1"): $(md5sum "$1" | cut -d" " -f1)"' _ {} \;
        
        # Create timestamp file to ensure changes are detected
        echo "=== Creating timestamp file ==="
        timestamp_file="jobs/.last_updated"
        
        echo "# Job Scraper Run Information" > "$timestamp_file"
        echo "Last updated: $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> "$timestamp_file"
        echo "GitHub Run ID: ${{ github.run_id }}" >> "$timestamp_file"
        echo "Workflow: ${{ github.workflow }}" >> "$timestamp_file"
        echo "Trigger: ${{ github.event_name }}" >> "$timestamp_file"
        echo "Branch: ${{ github.ref_name }}" >> "$timestamp_file"
        echo "" >> "$timestamp_file"
        
        echo "# Job Counts by Company:" >> "$timestamp_file"
        total_jobs=0
        for file in jobs/*.json; do
          if [ -f "$file" ]; then
            filename=$(basename "$file" _jobs_processed.json)
            if jq empty "$file" 2>/dev/null; then
              job_count=$(jq length "$file" 2>/dev/null || echo "0")
              echo "$filename: $job_count jobs" >> "$timestamp_file"
              total_jobs=$((total_jobs + job_count))
            else
              echo "$filename: invalid JSON" >> "$timestamp_file"
            fi
          fi
        done
        
        echo "" >> "$timestamp_file"
        echo "Total jobs across all companies: $total_jobs" >> "$timestamp_file"
        
        echo "✅ Created timestamp file with $total_jobs total jobs"
        
        # Clean up temporary artifacts directory
        rm -rf temp-artifacts/
        
    - name: List downloaded files
      run: |
        echo "📁 Final job files:"
        for file in jobs/*.json; do
          if [ -f "$file" ]; then
            filename=$(basename "$file")
            file_size=$(du -h "$file" | cut -f1)
            job_count=$(jq length "$file" 2>/dev/null || echo "0")
            echo "- $filename: $file_size ($job_count jobs)"
          fi
        done
        
    - name: Validate JSON files
      run: |
        echo "🔍 Validating JSON files..."
        for file in jobs/*.json; do
          if [ -f "$file" ]; then
            if jq empty "$file" 2>/dev/null; then
              echo "✅ $(basename "$file") is valid"
            else
              echo "❌ $(basename "$file") is invalid"
              exit 1
            fi
          fi
        done
        
    - name: Check for changes
      id: changes
      run: |
        # Stage all files in jobs directory
        git add jobs/
        
        if git diff --cached --quiet; then
          echo "no_changes=true" >> $GITHUB_OUTPUT
          echo "ℹ️ No changes detected"
        else
          echo "no_changes=false" >> $GITHUB_OUTPUT
          echo "✅ Changes detected in:"
          git diff --cached --name-only
        fi
        
    - name: Commit and push changes
      if: steps.changes.outputs.no_changes == 'false'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add jobs/
        git commit -m "🤖 Auto-update job listings - $(date +'%Y-%m-%d %H:%M:%S UTC')"
        git push origin main
        
    - name: Generate summary report
      run: |
        echo "## 📊 Job Scraping Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 📈 Job Counts by Company:" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        total_jobs=0
        for file in jobs/*_jobs_processed.json; do
          if [ -f "$file" ]; then
            company=$(basename "$file" _jobs_processed.json | sed 's/.*/\u&/')
            count=$(jq length "$file" 2>/dev/null || echo "0")
            echo "- **$company**: $count jobs" >> $GITHUB_STEP_SUMMARY
            total_jobs=$((total_jobs + count))
          fi
        done
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 📊 Total Jobs: $total_jobs" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 📅 Last Updated:" >> $GITHUB_STEP_SUMMARY
        echo "- $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.changes.outputs.no_changes }}" == "false" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "✅ **Changes detected and committed to repository**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 📝 Files Updated:" >> $GITHUB_STEP_SUMMARY
          for file in jobs/*.json; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              file_size=$(du -h "$file" | cut -f1)
              echo "- $filename ($file_size)" >> $GITHUB_STEP_SUMMARY
            fi
          done
        else
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ℹ️ **No new changes detected**" >> $GITHUB_STEP_SUMMARY
        fi
        
    - name: Upload combined job data
      uses: actions/upload-artifact@v4
      with:
        name: job-data-${{ github.run_id }}-combined
        path: jobs/
        retention-days: 7
        
  notify:
    needs: process-results
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Generate notification
      run: |
        if [ "${{ needs.process-results.result }}" == "success" ]; then
          echo "🎉 Job scraping completed successfully!"
          echo "📊 Check the summary above for job counts."
        else
          echo "❌ Job scraping failed!"
          echo "🔍 Check the logs for more details."
        fi